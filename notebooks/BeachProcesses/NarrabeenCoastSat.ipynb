{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *CoastSat*: example at Narrabeen-Collaroy, Australia\n",
    "\n",
    "### THIS NOTEBOOK CANNOT BE RUN ON THE BINDERHUB SERVER... THIS IS A NOTEBOOK THAT WE WILL RUN TOGETHER DURING THE LECTURE\n",
    "\n",
    "This example shows users how to extract time-series of shoreline change over the last 30+ years at their site of interest.\n",
    "There are five main steps:\n",
    "1. Retrieval of the satellite images of the region of interest from Google Earth Engine\n",
    "2. Shoreline extraction at sub-pixel resolution\n",
    "3. Intersection of the shorelines with cross-shore transects\n",
    "4. Tidal correction \n",
    "5. Time-series post-processing\n",
    "6. Validation against in situ surveys\n",
    "\n",
    "This software is described in details in the following publications: \n",
    "- Shoreline detection:                      https://doi.org/10.1016/j.envsoft.2019.104528\n",
    "- Accuracy assessment and applications:     https://doi.org/10.1016/j.coastaleng.2019.04.004\n",
    "- Beach slope estimation:                   https://doi.org/10.1029/2020GL088365\n",
    "\n",
    "## Initial settings\n",
    "\n",
    "Refer to the **Installation** section of the README for instructions on how to install the Python packages necessary to run the software, including Google Earth Engine Python API. If that step has been completed correctly, the following packages should be imported without any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "plt.ion()\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from coastsat import SDS_download, SDS_preprocess, SDS_shoreline, SDS_tools, SDS_transects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Retrieval of the images from GEE\n",
    "\n",
    "Define the region of interest (`polygon`), the date range (`dates`) and the satellite missions (`sat_list`) from which you wish to retrieve the satellite images. The images will be cropped on the Google Earth Engine server and only the region of interest will be downloaded as a .tif file. The files will stored in the directory defined in `filepath`. \n",
    "\n",
    "Make sure the area of your ROI is smaller than 100 km2 (if larger split it into smaller ROIs).\n",
    "\n",
    "The function `SDS_download.check_images_available(inputs)` will print the number of images available for your inputs. The Landsat images are divided in Tier 1 and Tier 2, only Tier 1 images can be used for time-series analysis.\n",
    "\n",
    "For Landsat, users can also choose between Collection 1 and Collection 2 with the `collection` variable. Note that from 1st Jan 2022 newly acquired Landsat images are only available in Collection 2, with Landsat 9 only available in Collection 2, so it's preferred that you use Collection 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region of interest (longitude, latitude)\n",
    "polygon = [[[151.2957545, -33.7012561],\n",
    "            [151.297557, -33.7388075],\n",
    "            [151.312234, -33.7390216],\n",
    "            [151.311204, -33.701399],\n",
    "            [151.2957545, -33.7012561]]] \n",
    "# it's recommended to convert the polygon to the smallest rectangle (sides parallel to coordinate axes)       \n",
    "polygon = SDS_tools.smallest_rectangle(polygon)\n",
    "# date range\n",
    "dates = ['2017-12-01', '2018-01-01']\n",
    "# satellite missions ['L5','L7','L8','L9','S2']\n",
    "sat_list = ['S2']\n",
    "# choose Landsat collection 'C01' or 'C02'\n",
    "collection = 'C02'\n",
    "# name of the site\n",
    "sitename = 'NARRA'\n",
    "# directory where the data will be stored\n",
    "filepath = os.path.join(os.getcwd(), 'data')\n",
    "# put all the inputs into a dictionnary\n",
    "inputs = {'polygon': polygon, 'dates': dates, 'sat_list': sat_list, 'sitename': sitename, 'filepath':filepath,\n",
    "         'landsat_collection': collection}\n",
    "\n",
    "# before downloading the images, check how many images are available for your inputs\n",
    "SDS_download.check_images_available(inputs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `SDS_download.retrieve_images(inputs)` retrives the satellite images from Google Earth Engine.\n",
    "\n",
    "By default, only Landsat Tier 1 Top-of-Atmosphere and Sentinel-2 Level-1C products are downloaded. \n",
    "\n",
    "In case you need to access Tier 2 images for qualitative analysis, you need to set `inputs['include_T2'] = True` before calling `retrieve_images`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs['include_T2'] = True\n",
    "metadata = SDS_download.retrieve_images(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you have already retrieved the images**, just load the metadata file by only running the section below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = SDS_download.get_metadata(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Shoreline extraction\n",
    "\n",
    "This section maps the position of the shoreline on the satellite images. The user can define the cloud threhold (`cloud_thresh`) and select the spatial reference system in which to output the coordinates of the mapped shorelines (`output_epsg`). See http://spatialreference.org/ to find the EPSG number corresponding to your local coordinate system. Make sure that your are using **cartesian coordinates and not spherical coordinates (lat,lon)** like WGS84. If unsure, use 3857 which is the web mercator projection (used by Google Maps).\n",
    "\n",
    "To quality control each shoreline detection and manually validate the mapped shorelines, the user has two options:\n",
    "1. Set the parameter `check_detection` to `True`: shows each mapped shoreline and user can accept it or discard it.\n",
    "2. Set the parameter `adjust_detection` to `True` allows the user to adjust the position of the shoreline by modifying the Otsu threshold interactively.\n",
    "\n",
    "Finally, to save a figure for each mapped shoreline as a .jpg in the folder */jpg_files/detection* set `save_figure` to `True`. \n",
    "\n",
    "The other parameters are for advanced users only (which understand what is going on in the background) and are described in the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = { \n",
    "    # general parameters:\n",
    "    'cloud_thresh': 0.5,        # threshold on maximum cloud cover\n",
    "    'dist_clouds': 300,         # ditance around clouds where shoreline can't be mapped\n",
    "    'output_epsg': 28356,       # epsg code of spatial reference system desired for the output\n",
    "    # quality control:\n",
    "    'check_detection': True,    # if True, shows each shoreline detection to the user for validation\n",
    "    'adjust_detection': False,  # if True, allows user to adjust the postion of each shoreline by changing the threhold\n",
    "    'save_figure': True,        # if True, saves a figure showing the mapped shoreline for each image\n",
    "    # [ONLY FOR ADVANCED USERS] shoreline detection parameters:\n",
    "    'min_beach_area': 1000,     # minimum area (in metres^2) for an object to be labelled as a beach\n",
    "    'min_length_sl': 500,       # minimum length (in metres) of shoreline perimeter to be valid\n",
    "    'cloud_mask_issue': False,  # switch this parameter to True if sand pixels are masked (in black) on many images  \n",
    "    'sand_color': 'default',    # 'default', 'latest', 'dark' (for grey/black sand beaches) or 'bright' (for white sand beaches)\n",
    "    'pan_off': False,           # True to switch pansharpening off for Landsat 7/8/9 imagery\n",
    "    # add the inputs defined previously\n",
    "    'inputs': inputs,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [OPTIONAL] Save .jpg of the preprocessed RGB images \n",
    "Saves .jpg files of the preprocessed satellite images (cloud masking + pansharpening/down-sampling) under *./data/sitename/jpeg_files\\preprocessed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SDS_preprocess.save_jpg(metadata, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [OPTIONAL] Digitize a reference shoreline\n",
    "Creates a reference shoreline which helps to identify outliers and false detections (HIGHLY RECOMMENDED). The reference shoreline is manually digitised by the user on one of the images. The parameter `max_dist_ref` defines the maximum distance from the reference shoreline (in metres) at which a valid detected shoreline can be. If you think that the default value of 100 m will not capture the full shoreline variability of your site, increase this value to an appropriate distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib qt\n",
    "settings['reference_shoreline'] = SDS_preprocess.get_reference_sl(metadata, settings)\n",
    "settings['max_dist_ref'] = 100 # max distance (in meters) allowed from the reference shoreline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch shoreline detection\n",
    "Extracts the 2D shorelines from the images in the spatial reference system specified by the user in `settings['output_epsg']`. The mapped shorelines are saved into `output.pkl` (under *./data/sitename*) and can also be saved as a `geojson` to be used in a GIS software (see below).\n",
    "\n",
    "If you see that the sand pixels on the images are not being identified, change the parameter `sand_color` from `default` to `dark` or `bright` depending on the color of your beach. There is also another classifier called `latest` which contains all the training data. These different classifiers only apply to Landsat imagery, for Sentinel-2 there is only one. A new classifier can also be trained as shown in this [example](https://github.com/kvos/CoastSat/blob/master/classification/train_new_classifier.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "output = SDS_shoreline.extract_shorelines(metadata, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then remove duplicates and images with inaccurate georeferencing (threhsold at 10m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = SDS_tools.remove_duplicates(output) # removes duplicates (images taken on the same date by the same satellite)\n",
    "output = SDS_tools.remove_inaccurate_georef(output, 10) # remove inaccurate georeferencing (set threshold to 10 m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For use in GIS applications, you can save the mapped shorelines as a GEOJSON layer which can be easily imported into QGIS for example. You can choose to save the shorelines as a collection of lines or points (sometimes the lines are crossing over so better to use points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import CRS\n",
    "geomtype = 'points' # choose 'points' or 'lines' for the layer geometry\n",
    "gdf = SDS_tools.output_to_gdf(output, geomtype)\n",
    "if gdf is None:\n",
    "    raise Exception(\"output does not contain any mapped shorelines\")\n",
    "gdf.crs = CRS(settings['output_epsg']) # set layer projection\n",
    "# save GEOJSON layer to file\n",
    "gdf.to_file(os.path.join(inputs['filepath'], inputs['sitename'], '%s_output_%s.geojson'%(sitename,geomtype)),\n",
    "                                driver='GeoJSON', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple plot of the mapped shorelines. The coordinates are stored in the output dictionnary together with the exact dates in UTC time, the georeferencing accuracy and the cloud cover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[15,8])\n",
    "plt.axis('equal')\n",
    "plt.xlabel('Eastings')\n",
    "plt.ylabel('Northings')\n",
    "plt.grid(linestyle=':', color='0.5')\n",
    "for i in range(len(output['shorelines'])):\n",
    "    sl = output['shorelines'][i]\n",
    "    date = output['dates'][i]\n",
    "    plt.plot(sl[:,0], sl[:,1], '.', label=date.strftime('%d-%m-%Y'))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Shoreline analysis\n",
    "\n",
    "In this section we show how to compute time-series of cross-shore distance along user-defined shore-normal transects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you have already mapped the shorelines**, just load the output file (`output.pkl`) by running the section below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(inputs['filepath'], sitename)\n",
    "with open(os.path.join(filepath, sitename + '_output' + '.pkl'), 'rb') as f:\n",
    "    output = pickle.load(f)\n",
    "# remove duplicates (images taken on the same date by the same satellite)\n",
    "output = SDS_tools.remove_duplicates(output)\n",
    "# remove inaccurate georeferencing (set threshold to 10 m)\n",
    "output = SDS_tools.remove_inaccurate_georef(output, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 options to define the coordinates of the shore-normal transects:\n",
    "\n",
    "**Option 1**: the user can interactively draw the shore-normal transects along the beach by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib qt\n",
    "# transects = SDS_transects.draw_transects(output, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2**: the user can load the transect coordinates (make sure the coordinate reference system (CRS) is the same as defined previously in `settings['output_epsg]`) from a .geojson file by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geojson_file = os.path.join(os.getcwd(), 'examples', 'NARRA_transects.geojson')\n",
    "# transects = SDS_tools.transects_from_geojson(geojson_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 3**: manually provide the coordinates of the transects (in the same CRS as defined in `settings['output_epsg]` as shown in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transects = dict([])\n",
    "transects['NA1'] = np.array([[ 342880, 6269180 ], [ 343144, 6269037 ]])\n",
    "transects['NA2'] = np.array([[ 342733, 6268783 ], [ 343008, 6268664 ]])\n",
    "transects['NA3'] = np.array([[ 342451, 6267913 ], [ 342746, 6267859 ]])\n",
    "transects['NA4'] = np.array([[ 342460, 6267042 ], [ 342759, 6267075 ]])\n",
    "transects['NA5'] = np.array([[ 342675, 6266241 ], [ 342936, 6266388 ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the location of the transects, make sure they are in the right location with the origin always landwards! Also check that the transects are intersecting the mapped shorelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[15,8], tight_layout=True)\n",
    "plt.axis('equal')\n",
    "plt.xlabel('Eastings')\n",
    "plt.ylabel('Northings')\n",
    "plt.grid(linestyle=':', color='0.5')\n",
    "for i in range(len(output['shorelines'])):\n",
    "    sl = output['shorelines'][i]\n",
    "    date = output['dates'][i]\n",
    "    plt.plot(sl[:,0], sl[:,1], '.', label=date.strftime('%d-%m-%Y'))\n",
    "for i,key in enumerate(list(transects.keys())):\n",
    "    plt.plot(transects[key][0,0],transects[key][0,1], 'bo', ms=5)\n",
    "    plt.plot(transects[key][:,0],transects[key][:,1],'k-',lw=1)\n",
    "    plt.text(transects[key][0,0]-100, transects[key][0,1]+100, key,\n",
    "                va='center', ha='right', bbox=dict(boxstyle=\"square\", ec='k',fc='w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we intersect 2D shorelines with the shore-normal transects to obtain time-series of cross-shore distance along each transect.\n",
    "\n",
    "One way is to simply compute the intersection as the median of the shoreline points within a certain range (`along_dist`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# along-shore distance over which to consider shoreline points to compute the median intersection\n",
    "settings_transects = {'along_dist':25}\n",
    "cross_distance = SDS_transects.compute_intersection(output, transects, settings_transects) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the shorelines can often be noisy (small loops, double intersections, etc), therefore a few parameters can be set to quality-control the intersections between tranects and shorelines with a more advanced function `compute_intersection_QC()`:\n",
    "- `along_dist`: (in metres),\n",
    "    alongshore distance to caluclate the intersection (median of points \n",
    "    within this distance). \n",
    "- `min_points`:  minimum number of shoreline points to calculate an intersection.\n",
    "- `max_std`: (in metres) maximum STD for the shoreline points within the alongshore range, \n",
    "    if STD is above this value a NaN is returned for this intersection.\n",
    "- `max_range`: (in metres) maximum RANGE for the shoreline points within the alongshore range, \n",
    "    if RANGE is above this value a NaN is returned for this intersection.\n",
    "- `min_chainage`: (in metres) furthest distance landward of the transect origin that an intersection is \n",
    "    accepted, beyond this point a NaN is returned.\n",
    "- `multiple_inter`: ('auto','nan','max') defines how to deal with multiple shoreline intersections\n",
    "- `auto_prc`: (value between 0 and 1) by default 0.1, percentage of the time that a multiple intersection needs to be present to use the max in auto mode\n",
    "\n",
    "Default parameters for the more quality-controlled intersections are provided below and should work in most cases (leave as it is if unsure).\n",
    "\n",
    "The `multiple_inter` setting helps to deal with multiple shoreline intersections along the same transect. This is quite common, for example when there is a lagoon behind the beach and the transect crosses two water bodies. The function will try to identify this cases and the user can choose whether to:\n",
    "- `'nan'`: always assign a NaN when there are multile intersections.\n",
    "- `'max'`: always take the max (intersection the furtherst seaward).\n",
    "- `'auto'`: let the function decide transect by transect, and if it thinks there are two water bodies, take the max.\n",
    "If `'auto'` is chosen, the `auto_prc` parameter will define when to use the max, by default it is set to 0.1, which means that the function thinks there are two water bodies if 10% of the time-series show multiple intersections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_transects = { # parameters for computing intersections\n",
    "                      'along_dist':          25,        # along-shore distance to use for computing the intersection\n",
    "                      'min_points':          3,         # minimum number of shoreline points to calculate an intersection\n",
    "                      'max_std':             15,        # max std for points around transect\n",
    "                      'max_range':           30,        # max range for points around transect\n",
    "                      'min_chainage':        -100,      # largest negative value along transect (landwards of transect origin)\n",
    "                      'multiple_inter':      'auto',    # mode for removing outliers ('auto', 'nan', 'max')\n",
    "                      'prc_multiple':         0.1,      # percentage of the time that multiple intersects are present to use the max\n",
    "                     }\n",
    "cross_distance = SDS_transects.compute_intersection_QC(output, transects, settings_transects) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the time-series of shoreline change along each transect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[15,8], tight_layout=True)\n",
    "gs = gridspec.GridSpec(len(cross_distance),1)\n",
    "gs.update(left=0.05, right=0.95, bottom=0.05, top=0.95, hspace=0.05)\n",
    "for i,key in enumerate(cross_distance.keys()):\n",
    "    if np.all(np.isnan(cross_distance[key])):\n",
    "        continue\n",
    "    ax = fig.add_subplot(gs[i,0])\n",
    "    ax.grid(linestyle=':', color='0.5')\n",
    "    ax.set_ylim([-50,50])\n",
    "    ax.plot(output['dates'], cross_distance[key]- np.nanmedian(cross_distance[key]), '-o', ms=6, mfc='w')\n",
    "    ax.set_ylabel('distance [m]', fontsize=12)\n",
    "    ax.text(0.5,0.95, key, bbox=dict(boxstyle=\"square\", ec='k',fc='w'), ha='center',\n",
    "            va='top', transform=ax.transAxes, fontsize=14)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a .csv file for Excel users\n",
    "out_dict = dict([])\n",
    "out_dict['dates'] = output['dates']\n",
    "for key in transects.keys():\n",
    "    out_dict[key] = cross_distance[key]\n",
    "df = pd.DataFrame(out_dict)\n",
    "fn = os.path.join(settings['inputs']['filepath'],settings['inputs']['sitename'],\n",
    "                  'transect_time_series.csv')\n",
    "df.to_csv(fn, sep=',')\n",
    "print('Time-series of the shoreline change along the transects saved as:\\n%s'%fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tidal correction\n",
    "\n",
    "This last section shows how to tidally-correct the time-series of shoreline change using time-series of tide level and an estimate of the beach slope.\n",
    "\n",
    "For this example, measured water levels for Sydney are stored in a csv file located [here](https://github.com/kvos/CoastSat/blob/master/examples/NARRA_tides.csv). When using your own file make sure that the dates are in UTC time, as the CoastSat shorelines are also in UTC, and the datum for the water levels is approx. Mean Sea Level.\n",
    "\n",
    "We assume that the beach slope at Narrabeen-Collaroy is 0.1 along all transects.\n",
    "\n",
    "**Note**: if you don't have measured water levels and beach slope, it is possible to obtain an estimate of the beach slope and time-series of modelled tide levels at the time of image acquisition from the [FES2014](https://www.aviso.altimetry.fr/es/data/products/auxiliary-products/global-tide-fes/description-fes2014.html) global tide model by using the [CoastSat.slope](https://github.com/kvos/CoastSat.slope) repository (see [this publication](https://doi.org/10.1029/2020GL088365) for more details, open acess preprint [here](https://www.essoar.org/doi/10.1002/essoar.10502903.1)). Instructions on how to install the global tide model are available [here](https://github.com/kvos/CoastSat.slope/blob/master/doc/FES2014_installation.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the measured tide data\n",
    "filepath = os.path.join(os.getcwd(),'examples','NARRA_tides.csv')\n",
    "tide_data = pd.read_csv(filepath, parse_dates=['dates'])\n",
    "dates_ts = [_.to_pydatetime() for _ in tide_data['dates']]\n",
    "tides_ts = np.array(tide_data['tide'])\n",
    "\n",
    "# get tide levels corresponding to the time of image acquisition\n",
    "dates_sat = output['dates']\n",
    "tides_sat = SDS_tools.get_closest_datapoint(dates_sat, dates_ts, tides_ts)\n",
    "\n",
    "# plot the subsampled tide data\n",
    "fig, ax = plt.subplots(1,1,figsize=(15,4), tight_layout=True)\n",
    "ax.grid(which='major', linestyle=':', color='0.5')\n",
    "ax.plot(tide_data['dates'], tide_data['tide'], '-', color='0.6', label='all time-series')\n",
    "ax.plot(dates_sat, tides_sat, '-o', color='k', ms=6, mfc='w',lw=1, label='image acquisition')\n",
    "ax.set(ylabel='tide level [m]',xlim=[dates_sat[0],dates_sat[-1]], title='Water levels at the time of image acquisition');\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply tidal correction using a linear slope and a reference elevation to which project all the time-series of cross-shore change (to get time-series at Mean Sea Level, set `reference_elevation` to 0. You also need an estimate of the beach slope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tidal correction along each transect\n",
    "reference_elevation = 0 # elevation at which you would like the shoreline time-series to be\n",
    "beach_slope = 0.1\n",
    "cross_distance_tidally_corrected = {}\n",
    "for key in cross_distance.keys():\n",
    "    correction = (tides_sat-reference_elevation)/beach_slope\n",
    "    cross_distance_tidally_corrected[key] = cross_distance[key] + correction\n",
    "    \n",
    "# store the tidally-corrected time-series in a .csv file\n",
    "out_dict = dict([])\n",
    "out_dict['dates'] = dates_sat\n",
    "for key in cross_distance_tidally_corrected.keys():\n",
    "    out_dict[key] = cross_distance_tidally_corrected[key]\n",
    "df = pd.DataFrame(out_dict)\n",
    "fn = os.path.join(settings['inputs']['filepath'],settings['inputs']['sitename'],\n",
    "                  'transect_time_series_tidally_corrected.csv')\n",
    "df.to_csv(fn, sep=',')\n",
    "print('Tidally-corrected time-series of the shoreline change along the transects saved as:\\n%s'%fn)\n",
    "\n",
    "# plot the time-series of shoreline change (both raw and tidally-corrected)\n",
    "fig = plt.figure(figsize=[15,8], tight_layout=True)\n",
    "gs = gridspec.GridSpec(len(cross_distance),1)\n",
    "gs.update(left=0.05, right=0.95, bottom=0.05, top=0.95, hspace=0.05)\n",
    "for i,key in enumerate(cross_distance.keys()):\n",
    "    if np.all(np.isnan(cross_distance[key])):\n",
    "        continue\n",
    "    ax = fig.add_subplot(gs[i,0])\n",
    "    ax.grid(linestyle=':', color='0.5')\n",
    "    ax.set_ylim([-50,50])\n",
    "    ax.plot(output['dates'], cross_distance[key]- np.nanmedian(cross_distance[key]), '-o', ms=6, mfc='w', label='raw')\n",
    "    ax.plot(output['dates'], cross_distance_tidally_corrected[key]- np.nanmedian(cross_distance[key]), '-o', ms=6, mfc='w', label='tidally-corrected')\n",
    "    ax.set_ylabel('distance [m]', fontsize=12)\n",
    "    ax.text(0.5,0.95, key, bbox=dict(boxstyle=\"square\", ec='k',fc='w'), ha='center',\n",
    "            va='top', transform=ax.transAxes, fontsize=14)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Time-series post-processing\n",
    "\n",
    "Finally, we can post-process the time-series of shoreline change to remove obvious outliers and spikes in the data and average the time-series over different time-frames (seasonally or monthly).\n",
    "\n",
    "To demonstrate these functionalities, we load a full time-series of shoreline change at Narrabeen from Landsat imagery between 1984 and 2021. These shorelines were mapped automatically without manual quality control, intersected with the transects and tidally-corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mapped shorelines from 1984\n",
    "filename_output = os.path.join(os.getcwd(),'examples','NARRA_output.pkl')\n",
    "with open(filename_output, 'rb') as f:\n",
    "    output = pickle.load(f) \n",
    "\n",
    "# plot the mapped shorelines\n",
    "fig = plt.figure(figsize=[15,8], tight_layout=True)\n",
    "plt.axis('equal')\n",
    "plt.xlabel('Eastings')\n",
    "plt.ylabel('Northings')\n",
    "plt.grid(linestyle=':', color='0.5')\n",
    "plt.title('%d shorelines mapped at Narrabeen from 1984'%len(output['shorelines']))\n",
    "for i in range(len(output['shorelines'])):\n",
    "    sl = output['shorelines'][i]\n",
    "    date = output['dates'][i]\n",
    "    plt.plot(sl[:,0], sl[:,1], '.', label=date.strftime('%d-%m-%Y'))\n",
    "for i,key in enumerate(list(transects.keys())):\n",
    "    plt.plot(transects[key][0,0],transects[key][0,1], 'bo', ms=5)\n",
    "    plt.plot(transects[key][:,0],transects[key][:,1],'k-',lw=1)\n",
    "    plt.text(transects[key][0,0]-100, transects[key][0,1]+100, key,\n",
    "             va='center', ha='right', bbox=dict(boxstyle=\"square\", ec='k',fc='w'))\n",
    "\n",
    "# load long time-series (1984-2021)\n",
    "filepath = os.path.join(os.getcwd(),'examples','NARRA_time_series_tidally_corrected.csv')\n",
    "df = pd.read_csv(filepath, parse_dates=['dates'])\n",
    "dates = [_.to_pydatetime() for _ in df['dates']]\n",
    "cross_distance = dict([])\n",
    "for key in transects.keys():\n",
    "    cross_distance[key] = np.array(df[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Despiking the time-series\n",
    "\n",
    "The tidally-corrected time-series of shoreline change obtained with the steps above may still contain some outliers (from cloud shadows, false detections etc). The function `SDS_transects.reject_outliers()` was developed to remove obvious outliers in the time-series, by removing the points that do not make physical sense in a shoreline change setting. \n",
    "\n",
    "For example, the shoreline can experience rapid erosion after a large storm, but it will then take time to recover and return to its previous state. Therefore, if the shoreline erodes/accretes suddenly of a significant amount (`max_cross_change`) and then immediately returns to its previous state, this spike does not make any physical sense and can be considered an outlier. Additionally, this funciton also checks that the Otsu thresholds used to map the shoreline are within the typical range defined by `otsu_threshold`, with values outside this range identified as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove outliers in the time-series (coastal despiking)\n",
    "settings_outliers = {'max_cross_change':   40,             # maximum cross-shore change observable between consecutive timesteps\n",
    "                     'otsu_threshold':     [-.5,0],        # min and max intensity threshold use for contouring the shoreline\n",
    "                     'plot_fig':           True,           # whether to plot the intermediate steps\n",
    "                    }\n",
    "cross_distance = SDS_transects.reject_outliers(cross_distance,output,settings_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the Otsu threholds for the shorelines mapped at this site to see if there are any obvious outliers and adjust the values of `otsu_threshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Otsu thresholds for the mapped shorelines\n",
    "fig,ax = plt.subplots(1,1,figsize=[12,5],tight_layout=True)\n",
    "ax.grid(which='major',ls=':',lw=0.5,c='0.5')\n",
    "ax.plot(output['dates'],output['MNDWI_threshold'],'o-',mfc='w')\n",
    "ax.axhline(y=-0.5,ls='--',c='r',label='otsu_threshold limits')\n",
    "ax.axhline(y=0,ls='--',c='r')\n",
    "ax.set(title='Otsu thresholds on MNDWI for the %d shorelines mapped'%len(output['shorelines']),\n",
    "       ylim=[-0.6,0.2],ylabel='otsu threshold')\n",
    "ax.legend(loc='upper left')\n",
    "fig.savefig(os.path.join(inputs['filepath'], inputs['sitename'], 'otsu_threhsolds.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Seasonal averaging\n",
    "\n",
    "The cell below shows how to calculate seasonal averages on the time-series with `SDS_transects.seasonal_average()` and plot them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "season_colors = {'DJF':'C3', 'MAM':'C1', 'JJA':'C2', 'SON':'C0'}\n",
    "for key in cross_distance.keys():\n",
    "    chainage = cross_distance[key]\n",
    "    # remove nans\n",
    "    idx_nan = np.isnan(chainage)\n",
    "    dates_nonan = [dates[_] for _ in np.where(~idx_nan)[0]]\n",
    "    chainage = chainage[~idx_nan] \n",
    "    \n",
    "    # compute shoreline seasonal averages (DJF, MAM, JJA, SON)\n",
    "    dict_seas, dates_seas, chainage_seas, list_seas = SDS_transects.seasonal_average(dates_nonan, chainage)\n",
    "    \n",
    "    # plot seasonal averages\n",
    "    fig,ax=plt.subplots(1,1,figsize=[14,4],tight_layout=True)\n",
    "    ax.grid(b=True,which='major', linestyle=':', color='0.5')\n",
    "    ax.set_title('Time-series at %s'%key, x=0, ha='left')\n",
    "    ax.set(ylabel='distance [m]')\n",
    "    ax.plot(dates_nonan, chainage,'+', lw=1, color='k', mfc='w', ms=4, alpha=0.5,label='raw datapoints')\n",
    "    ax.plot(dates_seas, chainage_seas, '-', lw=1, color='k', mfc='w', ms=4, label='seasonally-averaged')\n",
    "    for k,seas in enumerate(dict_seas.keys()):\n",
    "        ax.plot(dict_seas[seas]['dates'], dict_seas[seas]['chainages'],\n",
    "                 'o', mec='k', color=season_colors[seas], label=seas,ms=5)\n",
    "    ax.legend(loc='lower left',ncol=6,markerscale=1.5,frameon=True,edgecolor='k',columnspacing=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Monthly averaging\n",
    "\n",
    "The cell below shows how to calculate seasonal averages on the time-series with `SDS_transects.monthly_average()` and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "month_colors = plt.cm.get_cmap('tab20')\n",
    "for key in cross_distance.keys():\n",
    "    chainage = cross_distance[key]\n",
    "    # remove nans\n",
    "    idx_nan = np.isnan(chainage)\n",
    "    dates_nonan = [dates[_] for _ in np.where(~idx_nan)[0]]\n",
    "    chainage = chainage[~idx_nan] \n",
    "    \n",
    "    # compute shoreline monthly averages\n",
    "    dict_month, dates_month, chainage_month, list_month = SDS_transects.monthly_average(dates_nonan, chainage)\n",
    "    \n",
    "    # plot monthly averages\n",
    "    fig,ax=plt.subplots(1,1,figsize=[14,4],tight_layout=True)\n",
    "    ax.grid(b=True,which='major', linestyle=':', color='0.5')\n",
    "    ax.set_title('Time-series at %s'%key, x=0, ha='left')\n",
    "    ax.set(ylabel='distance [m]')\n",
    "    ax.plot(dates_nonan, chainage,'+', lw=1, color='k', mfc='w', ms=4, alpha=0.5,label='raw datapoints')\n",
    "    ax.plot(dates_month, chainage_month, '-', lw=1, color='k', mfc='w', ms=4, label='monthly-averaged')\n",
    "    for k,month in enumerate(dict_month.keys()):\n",
    "        ax.plot(dict_month[month]['dates'], dict_month[month]['chainages'],\n",
    "                 'o', mec='k', color=month_colors(k), label=month,ms=5)\n",
    "    ax.legend(loc='lower left',ncol=7,markerscale=1.5,frameon=True,edgecolor='k',columnspacing=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validation against surveys\n",
    "\n",
    "This section provides a validation of the satellite-derived shorelines at Narrabeen-Collaroy against the survey data that we used in the practical (available at http://narrabeen.wrl.unsw.edu.au/download/narrabeen/).\n",
    "\n",
    "The csv file called `Narrabeen_profiles.csv` is in the `./examples` folder. The code below is used to preprocess the data into time-series of shoreline change at a user-define contour level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "from scipy import stats\n",
    "from datetime import timedelta\n",
    "import pytz\n",
    "# read the csv file\n",
    "fp_datasets = os.path.join(os.getcwd(),'examples','Narrabeen_Profiles.csv')\n",
    "df = pd.read_csv(fp_datasets)\n",
    "pf_names = list(np.unique(df['Profile ID']))\n",
    "\n",
    "# select contour level\n",
    "contour_level = 0.7\n",
    "\n",
    "# initialise topo_profiles structure\n",
    "topo_profiles = dict([])\n",
    "for i in range(len(pf_names)):\n",
    "     # read dates\n",
    "    df_pf = df.loc[df['Profile ID'] == pf_names[i]]\n",
    "    dates_str = df['Date']\n",
    "    dates_unique = np.unique(dates_str)\n",
    "    # loop through dates\n",
    "    topo_profiles[pf_names[i]] = {'dates':[],'chainages':[]}\n",
    "    for date in dates_unique:\n",
    "        # extract chainage and elevation for that date\n",
    "        df_date = df_pf.loc[dates_str == date]\n",
    "        chainages = np.array(df_date['Chainage'])\n",
    "        elevations = np.array(df_date['Elevation'])\n",
    "        if len(chainages) == 0: continue\n",
    "        # use interpolation to extract the chainage at the contour level\n",
    "        f = interpolate.interp1d(elevations, chainages, bounds_error=False)\n",
    "        chainage_contour_level = f(contour_level)\n",
    "        topo_profiles[pf_names[i]]['chainages'].append(chainage_contour_level)\n",
    "        date_utc = pytz.utc.localize(datetime.strptime(date,'%Y-%m-%d'))\n",
    "        topo_profiles[pf_names[i]]['dates'].append(date_utc)\n",
    "\n",
    "# plot time-series\n",
    "fig = plt.figure(figsize=[15,8], tight_layout=True)\n",
    "gs = gridspec.GridSpec(len(topo_profiles),1)\n",
    "gs.update(left=0.05, right=0.95, bottom=0.05, top=0.95, hspace=0.05)\n",
    "for i,key in enumerate(topo_profiles.keys()):\n",
    "    ax = fig.add_subplot(gs[i,0])\n",
    "    ax.grid(linestyle=':', color='0.5')\n",
    "    ax.plot(topo_profiles[key]['dates'], topo_profiles[key]['chainages'], '-o', ms=4, mfc='w')\n",
    "    ax.set_ylabel('distance [m]', fontsize=12)\n",
    "    ax.text(0.5,0.95, key, bbox=dict(boxstyle=\"square\", ec='k',fc='w'), ha='center',\n",
    "            va='top', transform=ax.transAxes, fontsize=14)  \n",
    "# save a .pkl file\n",
    "with open(os.path.join(os.getcwd(), 'examples', 'Narrabeen_ts_07m.pkl'), 'wb') as f:\n",
    "    pickle.dump(topo_profiles, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare the surveys to the satellite-derived shorelines from 1984 loaded previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load survey data\n",
    "with open(os.path.join(os.getcwd(), 'examples', 'Narrabeen_ts_07m.pkl'), 'rb') as f:\n",
    "    gt = pickle.load(f)\n",
    "# change names to mach surveys\n",
    "for i,key in enumerate(list(cross_distance.keys())):\n",
    "    key_gt = list(gt.keys())[i]\n",
    "    cross_distance[key_gt] = cross_distance.pop(key)\n",
    "\n",
    "# set parameters for comparing the two time-series\n",
    "sett = {'min_days':3,   # numbers of days difference under which to use nearest neighbour interpolation\n",
    "        'max_days':10,  # maximum number of days difference to do a comparison\n",
    "        'binwidth':3,   # binwidth for histogram plotting\n",
    "        'lims':[-50,50] # cross-shore change limits for plotting purposes\n",
    "       }\n",
    "\n",
    "# initialise variables\n",
    "chain_sat_all = []\n",
    "chain_sur_all = [] \n",
    "satnames_all = []  \n",
    "for key in cross_distance.keys():\n",
    "    \n",
    "    # remove nans\n",
    "    chainage = cross_distance[key]\n",
    "    idx_nan = np.isnan(chainage)\n",
    "    dates_nonans = [output['dates'][k] for k in np.where(~idx_nan)[0]]\n",
    "    satnames_nonans = [output['satname'][k] for k in np.where(~idx_nan)[0]]\n",
    "    chain_nonans = chainage[~idx_nan]\n",
    "    \n",
    "    chain_sat_dm = chain_nonans\n",
    "    chain_sur_dm = gt[key]['chainages']\n",
    "    \n",
    "    # plot the time-series\n",
    "    fig= plt.figure(figsize=[15,8], tight_layout=True)\n",
    "    gs = gridspec.GridSpec(2,3)\n",
    "    ax0 = fig.add_subplot(gs[0,:])\n",
    "    ax0.grid(which='major',linestyle=':',color='0.5')\n",
    "    ax0.plot(gt[key]['dates'], chain_sur_dm, '-',mfc='w',ms=5,label='in situ')\n",
    "    ax0.plot(dates_nonans, chain_sat_dm,'-',mfc='w',ms=5,label='satellite')\n",
    "    ax0.set(title= 'Transect ' + key, xlim=[output['dates'][0]-timedelta(days=30),\n",
    "                                           output['dates'][-1]+timedelta(days=30)])#,ylim=sett['lims'])\n",
    "    ax0.legend()\n",
    "    \n",
    "    # interpolate surveyed data around satellite data\n",
    "    chain_int = np.nan*np.ones(len(dates_nonans))\n",
    "    for k,date in enumerate(dates_nonans):\n",
    "        # compute the days distance for each satellite date \n",
    "        days_diff = np.array([ (_ - date).days for _ in gt[key]['dates']])\n",
    "        # if nothing within 10 days put a nan\n",
    "        if np.min(np.abs(days_diff)) > sett['max_days']:\n",
    "            chain_int[k] = np.nan\n",
    "        else:\n",
    "            # if a point within 3 days, take that point (no interpolation)\n",
    "            if np.min(np.abs(days_diff)) < sett['min_days']:\n",
    "                idx_closest = np.where(np.abs(days_diff) == np.min(np.abs(days_diff)))\n",
    "                chain_int[k] = float(gt[key]['chainages'][idx_closest[0][0]])\n",
    "            else: # otherwise, between 3 and 10 days, interpolate between the 2 closest points\n",
    "                if sum(days_diff > 0) == 0:\n",
    "                    break\n",
    "                idx_after = np.where(days_diff > 0)[0][0]\n",
    "                idx_before = idx_after - 1\n",
    "                x = [gt[key]['dates'][idx_before].toordinal() , gt[key]['dates'][idx_after].toordinal()]\n",
    "                y = [gt[key]['chainages'][idx_before], gt[key]['chainages'][idx_after]]\n",
    "                f = interpolate.interp1d(x, y,bounds_error=True)\n",
    "                chain_int[k] = float(f(date.toordinal()))\n",
    "    \n",
    "    # remove nans again\n",
    "    idx_nan = np.isnan(chain_int)\n",
    "    chain_sat = chain_nonans[~idx_nan]\n",
    "    chain_sur = chain_int[~idx_nan]\n",
    "    dates_sat = [dates_nonans[k] for k in np.where(~idx_nan)[0]]\n",
    "    satnames = [satnames_nonans[k] for k in np.where(~idx_nan)[0]]\n",
    "    chain_sat_all = np.append(chain_sat_all,chain_sat)\n",
    "    chain_sur_all = np.append(chain_sur_all,chain_sur)\n",
    "    satnames_all = satnames_all + satnames\n",
    "    \n",
    "    # error statistics\n",
    "    slope, intercept, rvalue, pvalue, std_err = stats.linregress(chain_sur, chain_sat)\n",
    "    R2 = rvalue**2 \n",
    "    ax0.text(0,1,'R2 = %.2f'%R2,bbox=dict(boxstyle='square', facecolor='w', alpha=1),transform=ax0.transAxes)\n",
    "    chain_error = chain_sat - chain_sur\n",
    "    rmse = np.sqrt(np.mean((chain_error)**2))\n",
    "    mean = np.mean(chain_error)\n",
    "    std = np.std(chain_error)\n",
    "    q90 = np.percentile(np.abs(chain_error), 90)\n",
    "    \n",
    "    # 1:1 plot\n",
    "    ax1 = fig.add_subplot(gs[1,0])\n",
    "    ax1.axis('equal')\n",
    "    ax1.grid(which='major',linestyle=':',color='0.5')\n",
    "    for k,sat in enumerate(list(np.unique(satnames))): \n",
    "        idx = np.where([_ == sat for _ in satnames])[0]\n",
    "        ax1.plot(chain_sur[idx], chain_sat[idx], 'o', ms=4, mfc='C'+str(k),mec='C'+str(k), alpha=0.7, label=sat)\n",
    "    ax1.legend(loc=4)\n",
    "    ax1.plot([ax1.get_xlim()[0], ax1.get_ylim()[1]],[ax1.get_xlim()[0], ax1.get_ylim()[1]],'k--',lw=2)\n",
    "    ax1.set(xlabel='survey [m]', ylabel='satellite [m]')\n",
    "\n",
    "    # boxplots\n",
    "    ax2 = fig.add_subplot(gs[1,1])\n",
    "    data = []\n",
    "    median_data = []\n",
    "    n_data = []\n",
    "    ax2.yaxis.grid()\n",
    "    for k,sat in enumerate(list(np.unique(satnames))):\n",
    "        idx = np.where([_ == sat for _ in satnames])[0]\n",
    "        data.append(chain_error[idx])\n",
    "        median_data.append(np.median(chain_error[idx]))\n",
    "        n_data.append(len(chain_error[idx]))\n",
    "    bp = ax2.boxplot(data,0,'k.', labels=list(np.unique(satnames)), patch_artist=True)\n",
    "    for median in bp['medians']:\n",
    "        median.set(color='k', linewidth=1.5)\n",
    "    for j,boxes in enumerate(bp['boxes']):\n",
    "        boxes.set(facecolor='C'+str(j))\n",
    "        ax2.text(j+1,median_data[j]+1, '%.1f' % median_data[j], horizontalalignment='center', fontsize=12)\n",
    "        ax2.text(j+1+0.35,median_data[j]+1, ('n=%.d' % int(n_data[j])), ha='center', va='center', fontsize=12, rotation='vertical')\n",
    "    ax2.set(ylabel='error [m]', ylim=sett['lims'])\n",
    "    \n",
    "    # histogram\n",
    "    ax3 = fig.add_subplot(gs[1,2])\n",
    "    ax3.grid(which='major',linestyle=':',color='0.5')\n",
    "    ax3.axvline(x=0, ls='--', lw=1.5, color='k')\n",
    "    binwidth=sett['binwidth']\n",
    "    bins = np.arange(min(chain_error), max(chain_error) + binwidth, binwidth)\n",
    "    density = plt.hist(chain_error, bins=bins, density=True, color='0.6', edgecolor='k', alpha=0.5)\n",
    "    mu, std = stats.norm.fit(chain_error)\n",
    "    pval = stats.normaltest(chain_error)[1]\n",
    "    xlims = ax3.get_xlim()\n",
    "    x = np.linspace(xlims[0], xlims[1], 100)\n",
    "    p = stats.norm.pdf(x, mu, std)\n",
    "    ax3.plot(x, p, 'r-', linewidth=1)\n",
    "    ax3.set(xlabel='error [m]', ylabel='pdf', xlim=sett['lims'])   \n",
    "    str_stats = ' rmse = %.1f\\n mean = %.1f\\n std = %.1f\\n q90 = %.1f' % (rmse, mean, std, q90) \n",
    "    ax3.text(0, 0.98, str_stats,va='top', transform=ax3.transAxes)\n",
    "    \n",
    "    # save plot\n",
    "    fig.savefig(os.path.join(os.getcwd(),'examples','comparison_transect_%s.jpg'%key), dpi=150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the error statistics for all transects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate statistics for all transects together\n",
    "chain_error = chain_sat_all - chain_sur_all        \n",
    "slope, intercept, rvalue, pvalue, std_err = stats.linregress(chain_sur, chain_sat) \n",
    "R2 = rvalue**2\n",
    "rmse = np.sqrt(np.mean((chain_error)**2))\n",
    "mean = np.mean(chain_error)\n",
    "std = np.std(chain_error)\n",
    "q90 = np.percentile(np.abs(chain_error), 90)\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,5), tight_layout=True)\n",
    "# histogram\n",
    "ax[0].grid(which='major',linestyle=':',color='0.5')\n",
    "ax[0].axvline(x=0, ls='--', lw=1.5, color='k')\n",
    "binwidth=sett['binwidth']\n",
    "bins = np.arange(min(chain_error), max(chain_error) + binwidth, binwidth)\n",
    "density = ax[0].hist(chain_error, bins=bins, density=True, color='0.6', edgecolor='k', alpha=0.5)\n",
    "mu, std = stats.norm.fit(chain_error)\n",
    "pval = stats.normaltest(chain_error)[1]\n",
    "xlims = ax3.get_xlim()\n",
    "x = np.linspace(xlims[0], xlims[1], 100)\n",
    "p = stats.norm.pdf(x, mu, std)\n",
    "ax[0].plot(x, p, 'r-', linewidth=1)\n",
    "ax[0].set(xlabel='error [m]', ylabel='pdf', xlim=sett['lims'])   \n",
    "str_stats = ' rmse = %.1f\\n mean = %.1f\\n std = %.1f\\n q90 = %.1f' % (rmse, mean, std, q90) \n",
    "ax[0].text(0, 0.98, str_stats,va='top', transform=ax[0].transAxes,fontsize=14)\n",
    "\n",
    "# boxplot\n",
    "data = []\n",
    "median_data = []\n",
    "n_data = []\n",
    "ax[1].yaxis.grid()\n",
    "for k,sat in enumerate(list(np.unique(satnames_all))):\n",
    "    idx = np.where([_ == sat for _ in satnames_all])[0]\n",
    "    data.append(chain_error[idx])\n",
    "    median_data.append(np.median(chain_error[idx]))\n",
    "    n_data.append(len(chain_error[idx]))\n",
    "bp = ax[1].boxplot(data,0,'k.', labels=list(np.unique(satnames_all)), patch_artist=True)\n",
    "for median in bp['medians']:\n",
    "    median.set(color='k', linewidth=1.5)\n",
    "for j,boxes in enumerate(bp['boxes']):\n",
    "    boxes.set(facecolor='C'+str(j))\n",
    "    ax[1].text(j+1,median_data[j]+1, '%.1f' % median_data[j], horizontalalignment='center', fontsize=14)\n",
    "    ax[1].text(j+1+0.35,median_data[j]+1, ('n=%.d' % int(n_data[j])), ha='center', va='center', fontsize=12, rotation='vertical')\n",
    "ax[1].set(ylabel='error [m]', ylim=sett['lims']);\n",
    "fig.savefig(os.path.join(os.getcwd(),'examples','comparison_all_transects.jpg'), dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
